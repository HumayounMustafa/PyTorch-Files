{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTzRQ_ZIsJcT",
        "outputId": "338ac874-771e-4c5a-d4b7-dc8b79d00c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight,-0.4482830762863159, Gradient: -9818.3349609375\n",
            "linear.bias,-0.14838683605194092, Gradient: -147.1250762939453\n",
            "linear.weight,97.73506164550781, Gradient: 648157.1875\n",
            "linear.bias,1.3228639364242554, Gradient: 9674.15234375\n",
            "linear.weight,-6383.83642578125, Gradient: -42788048.0\n",
            "linear.bias,-95.41865539550781, Gradient: -638676.5\n",
            "linear.weight,421496.625, Gradient: 2824649728.0\n",
            "linear.bias,6291.34619140625, Gradient: 42162144.0\n",
            "linear.weight,-27825000.0, Gradient: -186469040128.0\n",
            "linear.bias,-415330.09375, Gradient: -2783330560.0\n",
            "linear.weight,1836865408.0, Gradient: 12309740126208.0\n",
            "linear.bias,27417974.0, Gradient: 183741366272.0\n",
            "linear.weight,-121260531712.0, Gradient: -812626535776256.0\n",
            "linear.bias,-1809995648.0, Gradient: -12129673412608.0\n",
            "linear.weight,8005004820480.0, Gradient: 5.364547726186906e+16\n",
            "linear.bias,119486734336.0, Gradient: 800739475587072.0\n",
            "linear.weight,-528449756200960.0, Gradient: -3.5414021584967434e+18\n",
            "linear.bias,-7887907717120.0, Gradient: -5.286074808218419e+16\n",
            "linear.weight,3.488557121404928e+16, Gradient: 2.3378541027838342e+20\n",
            "linear.bias,520719553265664.0, Gradient: 3.489598668154077e+18\n",
            "linear.weight,-2.302968473501827e+18, Gradient: -1.543332589034089e+22\n",
            "linear.bias,-3.437526682225869e+16, Gradient: -2.303655948644652e+20\n",
            "linear.weight,1.5203028201314556e+20, Gradient: 1.018829816092043e+24\n",
            "linear.bias,2.2692806741773517e+18, Gradient: 1.5207566070520341e+22\n",
            "linear.weight,-1.0036267265995523e+22, Gradient: -6.725796797919158e+25\n",
            "linear.bias,-1.4980638340652676e+20, Gradient: -1.0039262880323665e+24\n",
            "linear.weight,6.625434202721116e+23, Gradient: 4.4400297676625074e+27\n",
            "linear.bias,9.889456673542686e+21, Gradient: 6.6274120107392315e+25\n",
            "linear.weight,-4.373775131928543e+25, Gradient: -2.9310820175860584e+29\n",
            "linear.bias,-6.528517459316043e+23, Gradient: -4.3750807002403597e+27\n",
            "linear.weight,2.8873442974417704e+27, Gradient: 1.934951781642965e+31\n",
            "linear.bias,4.309795367120492e+25, Gradient: 2.8882061411308103e+29\n",
            "linear.weight,-1.9060782053797533e+29, Gradient: -1.2773568238487513e+33\n",
            "linear.bias,-2.8451080419147947e+27, Gradient: -1.9066472014283276e+31\n",
            "linear.weight,1.2582960531286334e+31, Gradient: 8.432461310319753e+34\n",
            "linear.bias,1.8781960314022757e+29, Gradient: 1.2586716663807876e+33\n",
            "linear.weight,-8.30663182348162e+32, Gradient: -5.566683519155633e+36\n",
            "linear.bias,-1.2398896739546728e+31, Gradient: -8.309110984101295e+34\n",
            "linear.weight,5.483616831395715e+34, Gradient: inf\n",
            "linear.bias,8.185121818925564e+32, Gradient: 5.485254081374072e+36\n",
            "linear.weight,-inf, Gradient: nan\n",
            "linear.bias,-5.4034027734341635e+34, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "linear.weight,nan, Gradient: nan\n",
            "linear.bias,nan, Gradient: nan\n",
            "Value is  tensor([nan], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "linear.weight tensor([[nan]], device='cuda:0')\n",
            "linear.bias tensor([nan], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "x=torch.tensor(np.arange(0,101),dtype=torch.float32,device='cuda').view(-1,1)\n",
        "y=torch.tensor(np.arange(1,102),dtype=torch.float32,device='cuda').view(-1,1)\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear=nn.Sequential(nn.Linear(1024,512),\n",
        "                              nn.ReLU(inplace=True),\n",
        "                              nn.Linear(512,256),\n",
        "                              nn.Linear(256,10)\n",
        "                              nn.SoftMax(inplace=True))\n",
        "    #self.linear2=nn.Linear(10,6)\n",
        "    #self.linear3=nn.Linear(6,1)\n",
        "  def forward(self,x):\n",
        "    #h=self.linear3(self.linear2(self.linear(x)))\n",
        "    return self.linear(x)\n",
        "\n",
        "model=LinearModel()\n",
        "model.to(device='cuda')\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=.01)\n",
        "num_epochs=100\n",
        "for i in range(num_epochs):\n",
        "  outputs=model(x)\n",
        "  loss=criterion(outputs,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f'{name},{param.data.item()}, Gradient: {param.grad.item()}')\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "test=torch.tensor([10.0],device='cuda')\n",
        "res=model(test)\n",
        "print(\"Value is \", res)\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "  print(name,param.data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "x=torch.tensor(np.arange(1,101),dtype=torch.float32).view(-1,1)\n",
        "y=torch.tensor(np.arange(2,102),dtype=torch.float32).view(-1,1)\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear=nn.Sequential(nn.Linear(1024,512),\n",
        "                              nn.ReLU(inplace=True)\n",
        "                              nn.Linear(512,256)\n",
        "                              nn.Sigmoid(inplace=True),\n",
        "                              nn.Linear(256,10),\n",
        "                              nn.Softmax())\n",
        "    #self.linear2=nnLinear(2,1)\n",
        "  [.1,.1,.2,93,0,0,0,0,0]\n",
        "  def forward(self,x):\n",
        "    return self.linear(x)\n",
        "    #output=self.linear(x)\n",
        "    #output2=self.linear(output)\n",
        "    #return output2\n",
        "\n",
        "model=LinearModel()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=.01)\n",
        "\n",
        "data=torch.utils.data.TensorDataset(x,y)\n",
        "dataloader=torch.utils.data.DataLoader(data,batch_size=2,shuffle=True)\n",
        "epochs=100\n",
        "for i in range(epochs):\n",
        "  for batch_x,batch_y in dataloader:\n",
        "    outputs=model(batch_x)\n",
        "    loss=criterion(outputs,batch_y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"After {i} epochs the loss is\",loss)\n",
        "\n",
        "test=torch.tensor([11],dtype=torch.float32)\n",
        "\n",
        "pred=model(test)\n",
        "print(f\"I have predicted this at {test}\", pred)\n",
        "for name,param in model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name,param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "PKKZfxkRQOgI",
        "outputId": "01f2197f-a267-487b-84d0-786345a32be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-41989e6fe810>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    nn.ReLU(inplace=True)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Sample data with 100 samples and 1 feature\n",
        "x = torch.tensor(np.arange(1, 101), dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor(np.arange(2, 102), dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the input and target data\n",
        "x_scaled = scaler_x.fit_transform(x)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Convert the scaled data back to tensors\n",
        "x = torch.tensor(x_scaled, dtype=torch.float32)\n",
        "y = torch.tensor(y_scaled, dtype=torch.float32)\n",
        "\n",
        "# Ensure that x and y have the same number of samples\n",
        "assert x.size(0) == y.size(0), \"Size mismatch between x and y\"\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Reduced learning rate\n",
        "\n",
        "data = torch.utils.data.TensorDataset(x, y)\n",
        "dataloader = torch.utils.data.DataLoader(data, batch_size=2, shuffle=True)\n",
        "\n",
        "epochs = 100  # Increased number of epochs\n",
        "\n",
        "# Use tqdm to track progress\n",
        "for i in tqdm(range(epochs), desc=\"Training\"):\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Make a prediction for a new input (e.g., 11)\n",
        "input_tensor = torch.tensor([11], dtype=torch.float32).view(-1, 1)\n",
        "# Scale the input using the MinMaxScaler\n",
        "input_scaled = scaler_x.transform(input_tensor)\n",
        "input_tensor = torch.tensor(input_scaled, dtype=torch.float32)\n",
        "prediction = model(input_tensor)\n",
        "# Inverse transform the prediction to get the original scale\n",
        "prediction = scaler_y.inverse_transform(prediction.detach().numpy())\n",
        "print(f\"I have predicted this at {input_tensor.item()}\", prediction[0, 0])\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li8rkhNrSsnb",
        "outputId": "333e0301-3f49-4f74-ee59-fdd4c9797e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:03<00:00, 30.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have predicted this at 0.10101009905338287 12.009506\n",
            "linear.weight tensor([[0.9998]])\n",
            "linear.bias tensor([0.0001])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a=torch.tensor([3.0],requires_grad=False)\n",
        "b=torch.tensor([2.0],requires_grad=False)\n",
        "c=a*b\n",
        "c.backward()\n",
        "#c*=2\n",
        "#c.backward()\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "QilWUYcxRCr0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "67a5d61d-96dd-48e7-d919-17313b36aae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-86f53b80345e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#c*=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#c.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1,2,3])\n",
        "b,c,d=a.detach()\n",
        "print(b.item(),c.item(),d.item())\n",
        "e=torch.(b,c,d)\n",
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "l3D4LTl0TFuh",
        "outputId": "819743e4-3720-4627-f5af-d128a0905eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7abf71d60c1d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'pack'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a=torch.tensor([3.0],requires_grad)"
      ],
      "metadata": {
        "id": "GeeKrJTlXkP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}